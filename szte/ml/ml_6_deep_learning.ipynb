{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdBl2YLZIdDI"
      },
      "source": [
        "# Deep Learning\n",
        "Olvasd el az [elméleti bevezetőt](http://inf.u-szeged.hu/~rfarkas/deep_learning.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_byfuGavovd"
      },
      "source": [
        "### Futtatás GPU-n\n",
        "\n",
        "A mély neurális hálók tanítása nagyon számításigényes, viszont visszavezetve mátrixműveletekre nagyon jól párhuzamosítható GPU-n. Érdemes a Google Colab-ban is átváltani GPU-ra. Ezt az Edit>Notebook settings menüben tehetjük meg GPU-t választva hardveres gyorsításra. Ha CPU-ról átvátunk GPU-ra akkor újra kell futtatni a teljes notebookot!\n",
        "\n",
        "A Cuda egy alacsony szintű szoftverréteg mátrixműveletek GPU-n való nagyon hatékony megvalósítására. E fölé épülnek a deep learning keretrendszerek. A két legelterjedtebb keretrendszer a [PyTorch](https://pytorch.org/) és a [Tensorflow](https://www.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q040XwLYboGA"
      },
      "source": [
        "### PyTorch deep learning keretrendszert használjuk: https://pytorch.org\n",
        "import torch  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7GPqKQoJEID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ff9a75-b8a2-48f1-9e92-9245cee0f223"
      },
      "source": [
        "### Futtatási környezet előkészítése\n",
        "\n",
        "# Cuda inicializálása\n",
        "torch.backends.cudnn.deterministic = True  \n",
        "\n",
        "# a neurális hálók tanításánál a véletlenszám-generálásnak nagy szerepe van\n",
        "# érdemes a random seedet fixálni, hogy minden futtatásra ugyanazt az eredményt kapjuk\n",
        "SEED = 202004\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIwFrOpCOI4b"
      },
      "source": [
        "# Szövegosztályozás mély tanulással\n",
        "\n",
        "A [5.](https://colab.research.google.com/drive/1iSIS8Z_iC8a4DjG4LE3MC04ZThcZPio-#scrollTo=SBHAiqDQoYfZ) órán megoldott szövegosztályozási feladatra fogunk adni itt egy mély gépi tanulási megoldást. Ugyanaz a feladat, véleményosztályozás. Ugyanazon az adatbázison, így az eredmények összehasonlíthatóak a klasszikus gépi tanulási eredményekkel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btOMn-h6F0AV"
      },
      "source": [
        "## Szózsák alapú neurális hálózat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM6TKWtgDVWa"
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/train.tsv', sep='\\t')\n",
        "test_data  = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/test.tsv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea7OfijBDfsk"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "vectorizer = CountVectorizer()\n",
        "cv_counts = vectorizer.fit_transform(train_data.text)\n",
        "idf_transformer = TfidfTransformer(use_idf=True).fit(cv_counts)\n",
        "features = idf_transformer.transform(cv_counts)\n",
        "test_features = idf_transformer.transform(vectorizer.transform(test_data.text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7hSKNsmEXew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2642af-8f4e-407b-f23b-4aef9306428f"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "model = SGDClassifier().fit(features, train_data.label)\n",
        "accuracy_score(y_true=test_data.label, y_pred=model.predict(test_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7893333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhMdy0QoGIEa"
      },
      "source": [
        "### ritka mátrixot tensor formátumra alakítjuk\n",
        "import numpy as np\n",
        "X_train_tensor = torch.from_numpy(features.todense()).float()\n",
        "X_test_tensor = torch.from_numpy(test_features.todense()).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tosH4W2lHXV1"
      },
      "source": [
        "### PyTorch-ban még a célváltozó sem lehet diszkrét...\n",
        "### A LabelEncoder véletlenszerűen Int-eket rendel az egyes értékekhez\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "Y_train_tensor = torch.as_tensor(le.fit_transform(train_data.label))\n",
        "Y_test_tensor  = torch.as_tensor(le.transform(test_data.label)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyjyKTgwKcwG"
      },
      "source": [
        "### Jellemzőtér (=bemeneti réteg) dimenziói és célváltozók száma (=kimeneti réteg dimenziója)\n",
        "VOCAB_SIZE = len(vectorizer.vocabulary_)\n",
        "OUT_CLASSES = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mthOAkd7IbcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5435a3-839c-4f9c-a848-cf23ad4c7d95"
      },
      "source": [
        "### A legegyszerűbb neurális háló (ami megegyezik a lineáris géppel)\n",
        "class LM_Network(torch.nn.Module):\n",
        "     def __init__(self,vocab_size,out_classes):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(vocab_size,out_classes)\n",
        "     def forward(self,x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = LM_Network(VOCAB_SIZE,OUT_CLASSES)\n",
        "print(model)\n",
        "\n",
        "#predikció a random hálóval\n",
        "model(X_train_tensor[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LM_Network(\n",
            "  (linear): Linear(in_features=24285, out_features=3, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0019, -0.0008,  0.0132], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC_eyopoJ4gU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f208a76-8296-4908-8d67-dc0862dbfbcb"
      },
      "source": [
        "### 1 rejtett réteget tartalmazó neuárlis hálózat\n",
        "class MLP_Network(torch.nn.Module):\n",
        "  def __init__(self,vocab_size,hidden_units,num_classes): \n",
        "      super().__init__()\n",
        "      #First fully connected layer\n",
        "      self.fc1 = torch.nn.Linear(vocab_size,hidden_units)\n",
        "      #Second fully connected layer\n",
        "      self.fc2 = torch.nn.Linear(hidden_units,num_classes)\n",
        "      #Final output of sigmoid function      \n",
        "      self.output = torch.nn.Sigmoid()\n",
        "  \n",
        "  def forward(self,x):\n",
        "      fc1 = self.fc1(x)\n",
        "      fc2 = self.fc2(fc1)\n",
        "      output = self.output(fc2)\n",
        "      return output\n",
        "\n",
        "HIDDEN_UNITS = 100\n",
        "model = MLP_Network(VOCAB_SIZE, HIDDEN_UNITS, OUT_CLASSES)\n",
        "print(model)\n",
        "\n",
        "#Prediction without training\n",
        "model(X_train_tensor[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_Network(\n",
            "  (fc1): Linear(in_features=24285, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
            "  (output): Sigmoid()\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5247, 0.4864, 0.4760], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWtXDPycDPqw"
      },
      "source": [
        "### Kiértékelő függvény \n",
        "\n",
        "def accuracy(preds, y):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # a 3 osztályra adott kimeneti érték közül melyik a legnagyobb\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum(dtype=float) / y.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kQU_qBAFXdD"
      },
      "source": [
        "### ha az epoch végén egy független validációs halmazon is ki akarjuk értékelni a modellt:\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "    epoch_acc = 0\n",
        "    model.eval()  # inicializálás \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            # predikció\n",
        "            predictions = model(batch[0])\n",
        "            # kiértékelés\n",
        "            acc = accuracy(predictions, batch[1].long())\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-S6Mn9Z0J3Z"
      },
      "source": [
        "from torch.utils.data import Dataset, TensorDataset\n",
        "train_data = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "test_data  = TensorDataset(X_test_tensor,  Y_test_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Togfr3NpkFlR"
      },
      "source": [
        "### Ha egy adatbázison akarunk végigmenni akkor ahhoz iterátort kell definiálni\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data,batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQni7CpQkLcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8802d39c-3c67-4fa9-f5c7-b3523123e168"
      },
      "source": [
        "# random hálózat kiértékelése az egész adatbázison\n",
        "evaluate(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33328609221466365"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nby2dP0FDq7R"
      },
      "source": [
        "### A tanítás során többször végigmegyünk a tanító adatbázison (egy kör egy epoch)\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    # minden epoch végén ellenőrízni fogjuk az accuracyt\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train() # inicializálás\n",
        "    for batch in iterator:\n",
        "        # predikáljuk le a tanító példákat az aktuális paraméterekkel:\n",
        "        optimizer.zero_grad()   \n",
        "        predictions = model(batch[0])\n",
        "                \n",
        "        # a háló aktuális paraméterivel ennyi a hiba a batchen:\n",
        "        loss = criterion(predictions, batch[1].long())\n",
        "        acc = accuracy(predictions, batch[1].long())   \n",
        "        \n",
        "        # hibavisszaterjesztéssel (backpropagation) javítunk a paramétereken:\n",
        "        loss.backward()\n",
        "        optimizer.step()      \n",
        "        \n",
        "        epoch_acc += acc.item()    \n",
        "      \n",
        "    return epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm9Vhb_EKlrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e815a7af-b23f-485f-8be0-959b0daa5811"
      },
      "source": [
        "### Neurális hálózat tanítása\n",
        "%%time\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#Neurális háló architektúra megadása\n",
        "model = MLP_Network(VOCAB_SIZE,HIDDEN_UNITS,OUT_CLASSES)\n",
        "\n",
        "#optimalizáló eljárás\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters()) # ADAM optimalizáló algoritmus\n",
        "\n",
        "#célfüggvény\n",
        "import torch.nn as nn\n",
        "loss_fun = nn.CrossEntropyLoss() \n",
        "\n",
        "iterator = DataLoader(train_data,batch_size=BATCH_SIZE, shuffle=True)\n",
        "for i in range(NUM_EPOCHS):\n",
        "   print(i, \". epoch acc:\", train(model, iterator, optimizer, loss_fun))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 . epoch acc: 0.6318475758396533\n",
            "1 . epoch acc: 0.8453864210906464\n",
            "2 . epoch acc: 0.9025172670639221\n",
            "3 . epoch acc: 0.9384677455760202\n",
            "4 . epoch acc: 0.9621168517515349\n",
            "CPU times: user 24.3 s, sys: 1.2 s, total: 25.5 s\n",
            "Wall time: 25.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXoX91PB7fs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1c4408-a476-4ade-b287-8c4134433fcd"
      },
      "source": [
        "### Kiértékelés a teszt halmazon\n",
        "test_loader = DataLoader(test_data,batch_size=16, shuffle=True)\n",
        "evaluate(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7799202127659575"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELuCBjMC80Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4089a61-5e1e-4e7b-ed80-ad1708d31ca8"
      },
      "source": [
        "### Futtassunk mindent GPU-n!\n",
        "%%time\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#Initialize model\n",
        "model = MLP_Network(VOCAB_SIZE,HIDDEN_UNITS,OUT_CLASSES).to(device)\n",
        "\n",
        "#Initialize optimizer\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters()) # ADAM optimalizáló algoritmus\n",
        "import torch.nn as nn\n",
        "loss_fun = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "X_train_tensor = X_train_tensor.to(device)\n",
        "Y_train_tensor = Y_train_tensor.to(device)\n",
        "train_data = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "iterator = DataLoader(train_data,batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "   print(i, \". epoch acc:\", train(model, iterator, optimizer, loss_fun))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 . epoch acc: 0.6946071460816179\n",
            "1 . epoch acc: 0.8498894005055977\n",
            "2 . epoch acc: 0.8993262459371614\n",
            "3 . epoch acc: 0.9361795774647887\n",
            "4 . epoch acc: 0.9607569293968942\n",
            "CPU times: user 4.51 s, sys: 1.37 s, total: 5.88 s\n",
            "Wall time: 17.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "299v4brZFU-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821c5362-27ea-4734-912b-23d1c66c6254"
      },
      "source": [
        "### mindent egyazon device-on kell futtatni...\n",
        "X_test_tensor = X_test_tensor.to(device)\n",
        "Y_test_tensor = Y_test_tensor.to(device)\n",
        "test_data = TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "test_loader = DataLoader(test_data,batch_size=16, shuffle=True)\n",
        "evaluate(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7768173758865248"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbdb66rAu_vC"
      },
      "source": [
        "## Szóbeágyazás alapú neurális hálók"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KZ3MbzBc8Rz"
      },
      "source": [
        "# torchtext egy szöveges adat feldolozására szolgáló segéd lib\n",
        "from torchtext.legacy import data  \n",
        "\n",
        "# a szöveges adat kezelésének módját lehet beállítani a torchtext-nek\n",
        "TEXT  = data.Field(tokenize = 'spacy') # A SpaCy egy hasonló csomag, mint az NLTK, annak a tokenizálója van beépítve\n",
        "# osztálycímke típusát állítjuk be:\n",
        "LABEL = data.LabelField(dtype = torch.float) # neurális hálóknál minden float (nincsen diszkrét célváltozó)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmhp3xoiefYI"
      },
      "source": [
        "### Adatbetöltés\n",
        "\n",
        "# Itt sajnos le kell töltenünk a fájlt\n",
        "import urllib \n",
        "url = 'https://github.com/rfarkas/student_data/raw/main/sentiment/train.tsv'\n",
        "urllib.request.urlretrieve(url,'train.tsv') # a temporálisan notebookhoz rendelt tárhelyre kerül a Google felhőjében\n",
        "\n",
        "# Definiálnunk kell, hogy melyik oszlop milyen típusú\n",
        "# első oszlop (ID) elhagyható, utána címke, végül a szöveg\n",
        "fields = [(None, None), ('label', LABEL), ('text',TEXT)]\n",
        "\n",
        "train_data = data.TabularDataset(path = 'train.tsv',\n",
        "                                 fields = fields, # oszlopok értelmezése\n",
        "                                 skip_header = True, # az első sorbeli oszlopneveket kihagyjuk. Az oszlopok neveit a fields-ben már megadtuk\n",
        "                                 format = 'tsv')  # a tsv formátum megadja a sep='\\t' is"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRTXPG38iWI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e563a9-5dfc-421c-d83d-baab91e54afa"
      },
      "source": [
        "# az adatbázis betöltésekor lefutnak a szövegelőfeldolgozó lépések is (most csak a spacy tokenizálás)\n",
        "print(vars(train_data.examples[0])) # examples[0] az első egyedünk\n",
        "print('Tanító példák száma:', len(train_data.examples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'NEGATIVE', 'text': ['HSBC', \"'\", 'sorry', \"'\", 'for', 'aiding', 'Mexican', 'drugs', 'lords', ',', 'rogue', 'states', 'and', 'terrorists', 'http://gu.com/p/394tx/tw', '\\xa0']}\n",
            "Tanító példák száma: 9063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLPmuBMiaJGW"
      },
      "source": [
        "### Kiértékelő adatbázis betöltése\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/rfarkas/student_data/main/sentiment/test.tsv'\n",
        "urllib.request.urlretrieve(url,'test.tsv')\n",
        "test_data = data.TabularDataset(path = 'test.tsv',\n",
        "                                    fields = fields, # oszlopok értelmezése\n",
        "                                    skip_header = True,\n",
        "                                    format = 'tsv')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAUFQ6UWmWs-"
      },
      "source": [
        "A gyorsabb működés érdekében a deep learning keretrendszerek fix méretű mátrixokkal dolgoznak. Az egyik fix méret a **szótár** mérete. A ritka szavakat inkább eldobjuk. Általában egy előre beállított szótármérettel dolgozunk, a leggyakoribb N szó kerül felhasználásra. A ritka szavak - amik nem férnek bele az N elemű szótárba - lecserélődnek `<unk>` (unknown) tokenre. A szótárat csak a tanító adatbázisból építjük! Ezzel is szimuláljuk, hogy ismeretlen példákon lehetnek ismeretlen szavak.\n",
        "\n",
        "A deep learning modellek ún. **szóbeágyazás**okat használnak tokenek leírására. Egy szóbeágyazás egy szóhoz egy numerikus vektort rendel. Ha két vektor közel van egymáshoz (pl. euklideszi vektortávolság szerint), akkor a két szó jelentése valamilyen értelemben hasonlít egymáshoz. Precízebben, két szóvektor akkor van közel egymáshoz ha hasonló mondatkörnyezetekben fordulnak elő. Egy fajta szóbeágyazás a [word2vec](https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953), de itt mi a [Glove](https://nlp.stanford.edu/projects/glove/) szóbeágyazást fogjuk használni, ami 6 millárd tokennyi szövegen ágyazta be az angol szavakat egy 100 dimenziós vektortérbe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kvowh16mG-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80221561-5c30-40d0-a974-447b2703a3c5"
      },
      "source": [
        "MAX_VOCAB_SIZE = 15000 # szótár mérete\n",
        "TEXT.build_vocab(train_data,\n",
        "                 max_size = MAX_VOCAB_SIZE,\n",
        "                 vectors = \"glove.6B.100d\") # ez először kitömörít 860MBot. A Google felhőjébe, nem lokálisan hozzád!\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [03:18, 4.34MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:27<00:00, 14430.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfzE7BIjnaYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4922dd-8a97-421a-e2aa-56b614123389"
      },
      "source": [
        "#print(\"Szótár mérete:\",len(TEXT.vocab))\n",
        "print(\"Osztálycímkék száma:\",len(LABEL.vocab))\n",
        "print(TEXT.vocab.freqs.most_common(10)) # leggyakorbb 10 szó és gyakoriságuk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Osztálycímkék száma: 3\n",
            "[('#', 4001), (',', 3967), ('.', 3873), ('\\xa0', 3660), ('the', 3339), (':', 3230), ('a', 3125), ('to', 2522), ('-', 2348), ('!', 2311)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItjaZVjioGUs"
      },
      "source": [
        "Szótár mérete 15002 mert a 15K szó mellé 2db technikai token is kerül, `<unk>` és `<pad>`. A szövegek hosszának is állandónak kell lennie a gyors mátrixműveletek érdekében. Ha a beállított fix szöveghossznál hosszabb szöveget adunk be, a hosszon túlllógó szavak elvesznek. Ha a fix hossznál rövidebb a szövegünk, akkor a hiányzó szöveget `<pad>` tokenekkel töltjük ki.\n",
        "\n",
        "Például ha 5 a fix szöveghossz, az 'I hate you' mondat tokensorozata az lesz, hogy `['I', 'hate', 'you', '<pad>', '<pad>']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVSr6lnDn4FO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a780d7-ab32-4324-8cd3-be695721ad1c"
      },
      "source": [
        "print(TEXT.vocab.itos[:10]) # szótár index szerinti első 10 eleme"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<pad>', '#', ',', '.', '\\xa0', 'the', ':', 'a', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSgPc2X1vVUg"
      },
      "source": [
        "## Konvolúciós Neurális Hálózat tanítása\n",
        "\n",
        "Egy ún **Konvulúciós Neurális Hálózatot** fogunk építani és tanítani a szövegosztályozási feladathoz (lásd [elméleti rész](TODO)). Egy bővebb PyTorch tutorial [itt](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5tIycnRph6S"
      },
      "source": [
        "# a gyors elosztott számításokhoz egyszerre több példával dolgozunk (batchek)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort=False,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3B8Y6_M6mAD"
      },
      "source": [
        "### Háló szerkezetének megadása\n",
        "\n",
        "Minden feladatra saját hálózatot építhetünk az egyes neuron rétegek megadásával. Ehhez egy új osztályt kell definiálni, legalább konstruktorral és forward() metódussal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awtIIIGcvQl3"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, kernel_size, output_dim, pad_idx):\n",
        "        super().__init__()\n",
        "        \n",
        "        # legalul a szóbeágyazások vannak, itt minden szót egy embedding_dim dimenziós vektor ír le, amit a Glove szótárból olvaunk ki\n",
        "        # <pad> speciálisan kezelődik (arra nincs értelme szóbeágyazás vektort használni) ezért meg kell adni az indexét\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        # utána jön a konvolúciós réteg (rétegek), itt a kernel mérete az \"ablakméret\"\n",
        "        self.conv = nn.Conv1d(in_channels = embedding_dim, \n",
        "                              out_channels = n_filters, \n",
        "                              kernel_size = kernel_size)\n",
        "        \n",
        "        # végül a kimeneti réteg, ami egy egyszerű lineáris réteg\n",
        "        self.fc = nn.Linear(n_filters, output_dim)\n",
        "                        \n",
        "    # amikor a szöveget előrefelé (\"alulról felfelé\" a rétegeken át) feldolgozza a háló\n",
        "    def forward(self, text):\n",
        "        \n",
        "        # a bemenet a batch_size darab szöveg, mindegyik pontosan sent_len hosszúságú\n",
        "        # text = [batch size, sent len]\n",
        "        text = text.permute(1, 0) # megcseréljük a dimenziókat\n",
        "        # text = [sent len, batch size]\n",
        " \n",
        "        # kiolvassuk a Glove szótárból a szóbeágyazási vektorokat\n",
        "        embedded = self.embedding(text)\n",
        "        # ez már egy 3 dimenziós tömb (tenzor):\n",
        "        # embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        #embedded = [batch size, emb dim, sent len]\n",
        "        \n",
        "        # ezután a konvolúciós réteg a RelU aktivációs függvényt használja\n",
        "        conved = F.relu(self.conv(embedded))\n",
        "            \n",
        "        # ennek a tenzornak a méretei:\n",
        "        # conved = [batch size, n_filters, sent len - filter_size + 1]\n",
        "        \n",
        "        # tovább tömörítjük: \n",
        "        pooled = F.max_pool1d(conved, conved.shape[2]).squeeze(2)\n",
        "        # pooled = [batch size, n_filters]\n",
        "\n",
        "        # a háló kimenetét a lineáris réteg számolja ki                    \n",
        "        return self.fc(pooled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI6KvHos_mpJ"
      },
      "source": [
        "### a háló példányosítása\n",
        "\n",
        "size_of_vocab = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "n_filters = 100\n",
        "kernel_size = 3\n",
        "output_dim = len(LABEL.vocab)\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = CNN(size_of_vocab, embedding_dim, n_filters, kernel_size, output_dim, pad_idx)                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0o_A0biBBOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e998dbc-4a6f-4cef-aee4-0eebd918761a"
      },
      "source": [
        "# a háló rétegei:\n",
        "print(model)\n",
        "\n",
        "# összesen ennyi paramétert kell tanítanunk:\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(count_parameters(model), \"tanulandó változó\") \n",
        "# ahhoz, hogy 1.5M válozót beállítsunk=megtanuljunk nagyon sok tanító példa kell(ene)!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (embedding): Embedding(15002, 100, padding_idx=1)\n",
            "  (conv): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n",
            "1530603 tanulandó változó\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGxRR8VzB0PI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fa4522-b620-4c89-a8ef-cfd763846add"
      },
      "source": [
        "### inicializáljuk a szóbeágyazási réteget\n",
        "### itt a Glove szótárból csak azoknak a szavaknak a vektorát olvassuk be, amire szükség van (a mi szótárunk)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([15002, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMF5u7GCCTLF"
      },
      "source": [
        "### CNN tanítása\n",
        "\n",
        "A neurális hálók tanítása egy optimalizációs feladat megoldásával törénik. Úgy akrjuk beállítani az 1.5M változót, hogy minimalizáljuk a háló kimenete és a tényleg osztálycímke közti eltérést."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4Zq489z56ok"
      },
      "source": [
        "### A tanítás során többször végigmegyünk a tanító adatbázison (egy kör egy epoch)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    # minden epoch végén ellenőrízni fogjuk az accuracyt\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train() # inicializálás\n",
        "    for batch in iterator:\n",
        "        # predikáljuk le a tanító példákat az aktuális paraméterekkel:\n",
        "        optimizer.zero_grad()   \n",
        "        predictions = model(batch.text)\n",
        "                \n",
        "        # a háló aktuális paraméterivel ennyi a hiba a batchen:\n",
        "        loss = criterion(predictions, batch.label.long())\n",
        "        acc = accuracy(predictions, batch.label)   \n",
        "        \n",
        "        # hibavisszaterjesztéssel (backpropagation) javítunk a paramétereken:\n",
        "        loss.backward()\n",
        "        optimizer.step()      \n",
        "        \n",
        "        epoch_acc += acc.item()    \n",
        "      \n",
        "    return epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne_iqphCCK2R"
      },
      "source": [
        "# GPU-n akarunk tanítani:\n",
        "model = CNN(size_of_vocab, embedding_dim, n_filters, kernel_size, output_dim, pad_idx)   \n",
        "model = model.to(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters()) # ADAM optimalizáló algoritmus\n",
        "criterion = nn.CrossEntropyLoss() # hibafüggvény többosztályos feladatokhoz\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlFwR_y-GOSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de432d8-1478-4a99-fd11-a9426ecd56c5"
      },
      "source": [
        "%%time\n",
        "### mehet a tanítás!\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "    print(i, \". epoch acc:\", train(model, train_iterator, optimizer, criterion))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 . epoch acc: 0.5599409759841097\n",
            "1 . epoch acc: 0.7653372156013001\n",
            "2 . epoch acc: 0.8459591684723726\n",
            "3 . epoch acc: 0.9043257945106536\n",
            "4 . epoch acc: 0.9483624503430842\n",
            "CPU times: user 5.05 s, sys: 88.6 ms, total: 5.14 s\n",
            "Wall time: 5.35 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afOLP_V0ZE7r"
      },
      "source": [
        "### CNN kiértékelése kiértékelő halmazon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF33EOXEREQh"
      },
      "source": [
        "### ha az epoch végén egy független validációs halmazon is ki akarjuk értékelni a modellt:\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "    epoch_acc = 0\n",
        "    model.eval()  # inicializálás \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            # predikció\n",
        "            predictions = model(batch.text)\n",
        "            # kiértékelés\n",
        "            acc = accuracy(predictions, batch.label.long())\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W73MgFlqZDz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "bd48d2fe-54f1-4d8f-ac4a-2564b11ad01c"
      },
      "source": [
        "evaluate(model, test_iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-354299c9346c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-df3ceb40d56a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# inicializálás\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;31m# predikció\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m    230\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LabelField' object has no attribute 'vocab'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyN-cvrHSlkQ"
      },
      "source": [
        "Mivel a tanító és kiértékelő adatbázisok valamint kiértékelési metrika megegyezzik az [5. órai](TODO) \"klasszikus\" gépi tanulási kísérletben használttal (79% accuracy), ezért az eredmények közvetlenül összehasonlíthatóak. Megállapíthatjuk, hogy ekkora tanító adatbázison a deep learning, még a szóbeágyazásokkal sem tud jobb eredményt elérni, mint \n",
        "a szózsák alapú lineáris gép."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvJt1wC8oS88"
      },
      "source": [
        "# Gyakorló fealdatok\n",
        "\n",
        "Futtasd az órai notebook-ot, hajtsd végre az alábbi módosításokat a rendszeren! \n",
        "\n",
        "1. Próbálj ki egy másik szóbeágyazást, úgy mennyi lesz a kiértékelő halmazon a pontosság?\n",
        "\n",
        "     Elérhető szóbeágyazásokhoz lásd [load_vector fgv leírása](https://torchtext.readthedocs.io/en/latest/vocab.html#vocab). Vigyázz, az embedding_dim változót is be kell állítani a használt beágyazás dimenziójára!\n",
        "\n",
        "2. Ha a konvolúció ablakméretét 5-re állítjuk, mennyi lesz a kiértékelő halmazon a pontosság?"
      ]
    }
  ]
}