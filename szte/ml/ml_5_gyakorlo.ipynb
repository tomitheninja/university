{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBHAiqDQoYfZ"
      },
      "source": [
        "# Gyakorló feladatok\n",
        "\n",
        "Az órai adatbázison hajts végre egy kísérletet (tanítás, predikció és kiértékelés) ahol\n",
        "\n",
        "*   a szavak szótövét vagy stemjét használjuk a szózsák modellben!\n",
        "*   egy másik lineáris gépet, a dLogisztkus Regresszió osztályozó algoritmust használunk (Logistic Regression Classifier).\n",
        "\n",
        "Írd ki, hogy mekkora szótár lesz így illetve mennyi így az accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlbBNbxRVpRv"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPATedleXOkz"
      },
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#pythonban egy sorban is megoldható :)\n",
        "# minden docs elemet tokenizálunk, minden tokenre stem()elünk, és a stemeket összefűzzük egy stringgé\n",
        "def stem_textcol(docs):\n",
        "  return [\" \".join([stemmer.stem(word) for word in nltk_tokenizer.tokenize(doc)]) for doc in docs] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qazL81BuXNqK"
      },
      "source": [
        "train_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/train.tsv', sep='\\t')\n",
        "test_data = pd.read_csv('https://github.com/rfarkas/student_data/raw/main/sentiment/test.tsv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmRGnOVJWpPJ"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4QYRxLrap4N"
      },
      "source": [
        "filtered_train_docs = stem_textcol(train_data.text)\n",
        "features = vectorizer.fit_transform(filtered_train_docs)\n",
        "print(\"Szótárméret: \", features.shape[1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UQY-MANldWG"
      },
      "source": [
        "logreg = LogisticRegression(max_iter=5000)\n",
        "model = logreg.fit(features, train_data.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw8W-NumYxUO"
      },
      "source": [
        "print(\"Train accuracy: \", accuracy_score(y_true=train_data.label, y_pred=model.predict(features)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2GgUUDpmhHl"
      },
      "source": [
        "filtered_test_docs = stem_textcol(test_data.text)\n",
        "test_features = vectorizer.transform(filtered_test_docs)\n",
        "print(\"Test accuracy: \", accuracy_score(y_true=test_data.label, y_pred=model.predict(test_features)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}